---
layout: post
title: 白话向量检索
description: 以最简单的方式解释向量是什么，怎么做向量检索，以及向量检索的应用场景。
use_mermaid: true
use_mathjax: true
---

# 什么是向量(Embedding)

向量是一个数据集合，用来表示一个事物的特征。比如，我们有一个班级的学生，每个学生有年龄、性别、身高、体重等特征，我们可以用一个向量来表示一个学生，
向量的每个维度对应一个特征，比如：[18, 1, 170, 60]，这个向量表示一个18岁、男性、身高170cm、体重60kg的学生。就这样我们可以把一些事物抽象出一些
特征属性来方便我们进行计算。比如我要找这个班级里面年龄最大的学生，那么我只需要找到年龄维度最大的向量就可以了。

我们可以把这种用向量表示一个事物的方法应用到更广泛的地方，比如图片，文本，音频等。比如我们有一张图片，我们可以用一个向量来表示这张图片，向量的每个维度
分别表示图片的一些特征，比如图片色阶直方图的分布，我们可以定量地表示这张图片的色彩分布，然后可以用这个向量来找到色彩相似的图片了。同样地，对于文本
数据，我们可以提取某个字出现的频次作为特征，这样就可以通过向量某个维度排序来找到某个字频次最高的一句话了。

当然，实际应用中我们不会这么简单地用一两个维度来来表示一个事物，我们会用更多的维度来表示，甚至使用了 "玄学" 来提取特征，如使用 CNN 的最后一层卷积层
作为特征向量。这些特征向量的提取方法不是本文的重点，不做展开。
这里推荐一个比较好玩的 CNN 可视化工具 [CNN Explainer](https://poloclub.github.io/cnn-explainer)。

# 如何做向量检索

假设我们有一个向量集合，我们要找到与某个向量最相似的向量，这个问题就是向量检索。一般地我们都是找数据集中与用户请求 Query 中最相似的向量，这种问题叫
TopN 检索。而向量 TopN 检索问题里面，分为 KNN(K-Nearest Neighbor) 和 ANN(Approximate Nearest Neighbor) 两种检索方式，KNN 是精确
检索，ANN 是近似检索。KNN 的检索较准确，但是计算量大，面对如今海量的数据处理起来不太科学。ANN 是一种近似的计算，可能会忽略掉最优的结果，但是给出的
结果页不会太差，而且计算量较小，适合大规模数据的检索。

向量检索问题其实就是这样，假设我们现在有一个向量集合来表示一堆文本的特征，现在需要找到与用户输入的文本最相似的文本，这个问题就是向量检索。现在我们有
的向量是：

```text
[100, 200, 100, 100]
[200, 300, 200, 500]
[300, 400, 300, 400]
[400, 500, 500, 600]
[500, 600, 600, 200]
```

这时候用户输入了一个文本，我们使用一样的特征算法得出这个文本的特征向量是 `[300, 200, 100, 400]`，我们需要找到这个向量和存量向量集合中距离最小的
向量。一般距离计算有余弦距离和欧氏距离两种。欧氏距离注重的是向量的绝对值，对于数值大小变化很敏感；而余弦距离注重的是向量的方向，对于数值相对大小比较
敏感。这里我们使用欧氏距离来算，因为比较简单，计算公式如下：

$ d = \sqrt{\sum\limits_{i=1}^{n} (x_{i} -y_{i})^{2} } $

对于两个四维的点 x、y，距离如下：

$ d(x, y) = \sqrt{(x_{1} - y_{1})^{2} + (x_{2} - y_{2})^{2} + (x_{3} - y_{3})^{2} + (x_{4} - y_{4})^{2}} $

## KNN 检索

KNN 检索的思路很简单，就是计算待检索向量与数据集中每个向量的距离，然后排序，取前 N 个最近的向量。这里的距离可以是欧式距离，也可以是余弦距离等。KNN
其实就是暴力计算，计算量大，不适合大规模数据的检索。

暴力计算对上述的例子来说很简单，我们只需要把用户检索的 Query 的特征向量和库里所有特征逐个计算距离然后排序取前 N 个结果即可。

$ d([100, 200, 100, 100], [300, 200, 100, 400]) = \sqrt{(100 - 300)^{2} + (200 - 200)^{2} + (100 - 100)^{2} + (100 - 400)^{2}} = 360.6 $
$ d([200, 300, 200, 500], [300, 200, 100, 400]) = \sqrt{(200 - 300)^{2} + (300 - 200)^{2} + (200 - 100)^{2} + (500 - 400)^{2}} = 200.0 $
$ d([300, 400, 300, 400], [300, 200, 100, 400]) = \sqrt{(300 - 300)^{2} + (400 - 200)^{2} + (300 - 100)^{2} + (400 - 400)^{2}} = 282.8 $
$ d([400, 500, 500, 600], [300, 200, 100, 400]) = \sqrt{(400 - 300)^{2} + (500 - 200)^{2} + (500 - 100)^{2} + (600 - 400)^{2}} = 547.7 $
$ d([500, 600, 600, 200], [300, 200, 100, 400]) = \sqrt{(500 - 300)^{2} + (600 - 200)^{2} + (600 - 100)^{2} + (200 - 400)^{2}} = 700.0 $

由此可得，与 `[300, 200, 100, 400]` 最相似的是 `[200, 300, 200, 500]`，其次是 `[300, 400, 300, 400]`，以此类推。

一般来说可以使用像 K-D 树等的方法构建高维空间的检索结构减少计算量，但是对于维度较高的向量效果有限，还是不适合做大规模的检索。

## ANN 检索

### 倒排

倒排索引在传统文本检索领域很常见，比如我们有一个文档集合，每个文档都有一个 ID，我们需要根据某个关键词找到对应的文档，除了暴力遍历文档集合，我们还可以
构建一个倒排索引，这个索引是文档中所有分词对应的文档 ID 集合，这样我们就可以快速找到对应的文档了。举例，现在我们有一些文档如下：

```text
doc_1: "我爱北京天安门"
doc_2: "我爱北京故宫"
doc_3: "我爱北京天坛"
```

这时对这些文档做倒排索引，我们可以得到如下的索引：

```text
我: [doc_1, doc_2, doc_3]
爱: [doc_1, doc_2, doc_3]
北京: [doc_1, doc_2, doc_3]
天安门: [doc_1]
故宫: [doc_2]
天坛: [doc_3]
```

如果用户输入 "天安门" 这个关键词，则可以找到 doc_2 这个文档，而不需要暴力检索所有文档了。

倒排本质是缩小检索范围。在向量检索中，我们也可以通过类似的方法来缩小检索范围。但是难点在于，向量是连续的，而文本是离散的，文本数据是有穷的，而向量数据
是无穷的，不能像文本检索那样直接用一个哈希表来存储。这时候我们可以使用聚类中心的方法来实现倒排。聚类中心的本质是把向量空间划分成若干个区域，相当于把
连续的数据离散化，这些在数字信号处理很常用。

首先解释一下什么是聚类中心，比如拿前面的数据，我们为了方便只拿前 2 个维度的数据作为例子，我们可以把这些数据画在二维坐标系中，如下图所示：

![聚类中心示意图]({{ "/assets/images/2023-06-03-cluster-example.png" | relative_url }})

如上图， 我们把这 5 个向量分成 2 组数据（多少组是可以指定的，根据数据量和期望的准确率来决定），每组数据有一个聚类中心，即 $ P_{1} $、$ P_{2} $、
$ P_{3} $ 的聚类中心是 $ P_{2} $，$ P_{4} $、$ P_{5} $ 的聚类中心是 $ P_{5} $，这个聚类中心即是这组数据的代表，聚类中心到这组数据的其他点
的距离是最短的。

关于聚类的方法，常用的是 [K-means](https://en.wikipedia.org/wiki/K-means_clustering) 算法，即通过多轮迭代无限找到接近最优解的聚类中心，
这里不做展开。

通过聚类我们就把连续的数据离散化了，这样就可以用哈希表来存储了，比如我们可以用 $ P_{2} $ 的哈希表存储 $ P_{1} $、$ P_{2} $、 $ P_{3} $ 了。
此时我们检索只需要把用户输入的 Query 和聚类中心进行比较即可。这里还有一个问题，假设我期望 返回 TOP3 的结果，用户输入的 Query 特征向量是
`(300, 400)`，期望的结果应该是 $ P_{2} $、$ P_{3} $、$ P_{4} $，但是由于我们锁定了 1 个聚类中心，导致结果是 $ P_{1} $、$ P_{2} $、
$ P_{3} $。通常地，我们会多搜索几个聚类中心里面的结果再排序来得到更优解。

```text
cluster_1(200, 300): [(100, 200), (200, 300), (300, 400)]
cluster_2(500, 600): [(400, 500), (500, 600)]
``` 

### HNSW

HNSW 是一种基于图的 ANN 检索算法，其全称是 [Hierarchical Navigable Small World](https://arxiv.org/abs/1603.09320)。HNSW 的思想是
通过构建一个图来实现 ANN 检索，这个图是一个稠密图，每个节点都有很多边，这样就可以快速找到相似的节点了。

了解 HNSW 前要先了解 NSW。NSW 的想法是通过构建一个无向图来表示向量之间的相似图，我们使用上面的向量后面 2 维做例子：

![聚类中心示意图]({{ "/assets/images/2023-06-03-nsw-example.png" | relative_url }})

这个图的实际上是 `m=2`，即构建时每个点都会连接离自己最近的 2 个点，这样就构建了一个无向图，因此所有点的度至少都是 2。假设我们现在有一个点
Q(400, 200)，需要找到离它最近的点。我们先随机找到一个点 $ P_{1} $，然后找到 $ P_{1} $ 的邻居点 $ P_{2} $ 和 $ P_{3} $，显然 $ P_{3} $
离 Q 点更近。然后找到 $ P_{3} $ 的邻居点 $ P_{1} $、$ P_{2} $、$ P_{4} $ 和 $ P_{5} $，显然 $ P_{5} $ 离 Q 点更近。染好找到
$ P_{5} $ 的邻居点  $ P_{3} $ 和 $ P_{4} $，发现距离变大了，遍历结束。这样我们就找到了离 Q 点最近的点 $ P_{5} $。

看完 NSW 后我们看一下 HNSW。HNSW 有点类似跳表，我们拿出论文里的例子图：

![聚类中心示意图]({{ "/assets/images/2023-06-03-hnsw-chart.png" | relative_url }})

HNSW 的思想是每层随机选出一些点来组成这一层的图，从下往上看即每层都会比前一层少一些点。构建的时候这个点属于哪一层使用随机函数来决定，这个随机函数是

$ f(x) = min(\lfloor -ln(unif(0..1)) * m_{L} \rfloor, L) $。$ unif(0..1) $ 是 0 到 1 之间均匀分布的随机函数，$ -ln(x) $ 的函数图像如下：

![聚类中心示意图]({{ "/assets/images/2023-06-03-fx-lnx.png" | relative_url }})

而 $ m_{L} $ 和 $ L $ 是用户输入的常数值。因此，一个点属于哪一层是随机的，而且往往是更大概率属于更下面的层。如果这个点属于第 $ l $ 层，那么他
一定属于第 $ [0, l-1] $ 层。

检索时从上往下遍历，因为有了更少的点所以可以更快逼近到期望值。

### PQ 量化

PQ(Product Quantization) 量化是一种将高维向量量化为低维向量的方法，其思想是将一个高维向量切分为多个子向量，然后对每个子向量进行聚类，并通过使用
码本的方法减少向量存储在内存中的大小。可以说 PQ 量化的模板是减少计算量和内存消耗。还是使用之前的例子，我们把 4 维向量切分成 2 组 2 维向量，每组进行
聚类，聚类分组每组也是 2 个。即：

```text
# 纵向切分向量，降低维度
[100, 200, 100, 100] -> [100, 200], [100, 100]
[200, 300, 200, 500] -> [200, 300], [200, 500]
[300, 400, 300, 400] -> [300, 400], [300, 400]
[400, 500, 500, 600] -> [400, 500], [500, 600]
[500, 600, 600, 200] -> [500, 600], [600, 200]

# 每组分别进行聚类，聚类中心 2 个
# 组1， cluster 1，聚类中心是 (200, 300)
[100, 200]
[200, 300]
[300, 400]
# 组2，cluster 2，聚类中心是 (450, 550)
[400, 500]
[500, 600]
# 组2，cluster 2，聚类中心是 
```


上面几种方法不一定要单独使用，可以组合使用。比如我们可以使用 HNSW + 倒排组合，先通过倒排找到一些候选集合，然后通过 HNSW 比较准确地找到候选点，这个
其实是华为云检索服务的做法。也可以通过二级聚类空间的方法，就是通过两次倒排检索找到结果，这个其实是百度
[GNOIMI](https://zhuanlan.zhihu.com/p/371343109) 的做法。

# 应用

## FAISS
